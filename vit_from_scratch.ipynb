{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb49ee5-d5a6-48f5-90b2-c3b8af8cf702",
   "metadata": {},
   "source": [
    "# Vision Transformer encoder from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bece6f-4e86-4fca-9dec-883c952d2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from PIL import Image\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "# from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Normalize, Resize, ToTensor\n",
    "\n",
    "from utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0361b7-57fa-4956-ac83-38dc4ebb23bf",
   "metadata": {},
   "source": [
    "### Patch embedding\n",
    "\n",
    "Using the rearange method, we have:\n",
    "\n",
    "* `b` the number of batches, here its one\n",
    "* `c` the number of channels, here its $3$\n",
    "* `(h s1)` is the total height ($224$), `s1` is the patch size ($16$) therefore `h` is $224/16$ which is $14$ \n",
    "* `(w s2)` is the total width ($224$), `s2` is the patch size ($16$) therefore `w` is $224/16$ which is $14$ \n",
    "\n",
    "We want to flatten all the patches.\n",
    "That means we will end up with:\n",
    "\n",
    "* `b`still the number of batch\n",
    "* `(h w)` the number of patches, therefore $14 * 14 = 196$ patches\n",
    "* `(s1 s2 c)` the size of the patches multiplied by the number of channels, that means each patch will be $768$ pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a77cce2-0d1c-47c6-afe3-3b0fdfe5d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        img_size: int,\n",
    "        patch_size: int = 16,\n",
    "        emb_size: int = 512,\n",
    "        is_SPT: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            img_size % patch_size == 0\n",
    "        ), f\"{img_size=}, needs to be a multiple of {patch_size=}\"\n",
    "\n",
    "        if is_SPT:\n",
    "            self.patch_shifting = PatchShifting(patch_size)\n",
    "            in_channels *= 5\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.to_patch = Rearrange(\n",
    "            \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_size, p2=patch_size\n",
    "        )\n",
    "        self.linear = nn.Linear(patch_size**2 * in_channels, emb_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(\n",
    "            torch.randn((img_size // patch_size) ** 2 + 1, emb_size)\n",
    "        )\n",
    "        self.is_SPT = is_SPT\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        if self.is_SPT:\n",
    "            x = self.patch_shifting(x)\n",
    "\n",
    "        x = self.to_patch(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = repeat(self.cls_token, \"() n e -> b n e\", b=batch_size)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.positions\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9785ca0b-9242-4636-9aa6-b12b50efe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchShifting(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.shift = int(patch_size * (1 / 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_pad = torch.nn.functional.pad(\n",
    "            x, (self.shift, self.shift, self.shift, self.shift)\n",
    "        )\n",
    "        # if self.is_mean:\n",
    "        #     x_pad = x_pad.mean(dim=1, keepdim = True)\n",
    "\n",
    "        \"\"\" 4 cardinal directions \"\"\"\n",
    "        #############################\n",
    "        # x_l2 = x_pad[:, :, self.shift:-self.shift, :-self.shift*2]\n",
    "        # x_r2 = x_pad[:, :, self.shift:-self.shift, self.shift*2:]\n",
    "        # x_t2 = x_pad[:, :, :-self.shift*2, self.shift:-self.shift]\n",
    "        # x_b2 = x_pad[:, :, self.shift*2:, self.shift:-self.shift]\n",
    "        # x_cat = torch.cat([x, x_l2, x_r2, x_t2, x_b2], dim=1)\n",
    "        #############################\n",
    "\n",
    "        \"\"\" 4 diagonal directions \"\"\"\n",
    "        # #############################\n",
    "        x_lu = x_pad[:, :, : -self.shift * 2, : -self.shift * 2]\n",
    "        x_ru = x_pad[:, :, : -self.shift * 2, self.shift * 2 :]\n",
    "        x_lb = x_pad[:, :, self.shift * 2 :, : -self.shift * 2]\n",
    "        x_rb = x_pad[:, :, self.shift * 2 :, self.shift * 2 :]\n",
    "        out = torch.cat([x, x_lu, x_ru, x_lb, x_rb], dim=1)\n",
    "        # #############################\n",
    "\n",
    "        \"\"\" 8 cardinal directions \"\"\"\n",
    "        #############################\n",
    "        # x_l2 = x_pad[:, :, self.shift:-self.shift, :-self.shift*2]\n",
    "        # x_r2 = x_pad[:, :, self.shift:-self.shift, self.shift*2:]\n",
    "        # x_t2 = x_pad[:, :, :-self.shift*2, self.shift:-self.shift]\n",
    "        # x_b2 = x_pad[:, :, self.shift*2:, self.shift:-self.shift]\n",
    "        # x_lu = x_pad[:, :, :-self.shift*2, :-self.shift*2]\n",
    "        # x_ru = x_pad[:, :, :-self.shift*2, self.shift*2:]\n",
    "        # x_lb = x_pad[:, :, self.shift*2:, :-self.shift*2]\n",
    "        # x_rb = x_pad[:, :, self.shift*2:, self.shift*2:]\n",
    "        # x_cat = torch.cat([x, x_l2, x_r2, x_t2, x_b2, x_lu, x_ru, x_lb, x_rb], dim=1)\n",
    "        #############################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d21b59-90b5-4902-87ce-9b2f748c5a61",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "the attention takes three inputs, the famous queries, keys, and values, and computes the attention matrix using queries and values and use it to “attend” to the values.\n",
    "\n",
    "First step is to split key, queries and values, we will go step by step for queries, keys and values follow the same pattern.\n",
    "* The input go trough a linear layer\n",
    "* we rearange the input\n",
    "    * `b` the batch size\n",
    "    * `n` the number of patches\n",
    "    * `(h d)` the size of one patch where `h` is the number of heads so `d` is the size of a patch given to an attention head.\n",
    "    * We rearange that into, the same batch size, but now we have `h` patches that each will handle the full amount of patches but of size `d`\n",
    "    \n",
    "Now that we have our `keys` `values` and `queries` in the right shape, we can perform a `matrix multiplication` between the `keys` and the `queries`.\n",
    "\n",
    "The attention map is then the softmax of this resulting matrix divided by some scaling.\n",
    "\n",
    "We parform a matrix multiplication and rearange the result back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2d12b1-14a1-4dc9-a23d-7f4172bb3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        emb_size: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "        self.scaling = emb_size ** (1 / 2)\n",
    "        self.is_LSA = is_LSA\n",
    "\n",
    "        if is_LSA:\n",
    "            num_patches = 5\n",
    "            self.scaling = nn.Parameter(self.scaling * torch.ones(1))\n",
    "            self.mask = torch.eye(num_patches, num_patches)\n",
    "            self.mask = torch.nonzero((self.mask == 1), as_tuple=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # split keys, queries and values in num_heads\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "\n",
    "        # sum up over the last axis\n",
    "        attention_weights = torch.einsum(\n",
    "            \"bhqd, bhkd -> bhqk\", queries, keys\n",
    "        )  # batch, num_heads, query_len, key_len\n",
    "\n",
    "        if self.is_LSA:\n",
    "            attention_weights[:, :, self.mask[:, 0], self.mask[:, 1]] = -987654321\n",
    "\n",
    "        attention_weights = F.softmax(attention_weights / self.scaling, dim=-1)\n",
    "        attention_weights = self.att_drop(attention_weights)\n",
    "\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum(\"bhal, bhlv -> bhav \", attention_weights, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")  # concat the heads\n",
    "        out = self.projection(out)  # project out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb45f3-c5f5-4dc2-9064-3416b4892710",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f310bb12-3e3b-4d37-b5f5-cf8ce1db9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer Encoder Block is a layerNorm foloowed by a MultiHeadAttention,\n",
    "    another layerNorm and finally a linearLayer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, emb_size: int = 512, num_heads: int = 8, is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            emb_size=emb_size, num_heads=num_heads, is_LSA=is_LSA\n",
    "        )\n",
    "        self.layer_norm_1 = LayerNorm(emb_size)\n",
    "        self.layer_norm_2 = LayerNorm(emb_size)\n",
    "        dropout = 0.1\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * emb_size, emb_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        residual = x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c73ff33f-29a0-491b-9328-e6082aaed88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Consists of `depth` encoder blocks chained together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth: int = 1,\n",
    "        emb_size: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(emb_size=emb_size, num_heads=num_heads, is_LSA=is_LSA)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491994e-4190-46de-b12e-6982c406122b",
   "metadata": {},
   "source": [
    "### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b286f2d-a1fb-4a76-9e57-6946f6517035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, *, emb_size: int = 512, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce(\n",
    "                \"b n e -> b e\", reduction=\"mean\"\n",
    "            ),  # Reduce the N patches into One patch by averaging them\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eef675-cb09-44ce-b7cd-d53985fbb990",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a41cc707-353c-4a92-952f-76b22c54d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        img_size: int,\n",
    "        n_classes: int = 10,\n",
    "        patch_size: int = 8,\n",
    "        emb_size: int = 192,\n",
    "        num_heads=12,\n",
    "        depth: int = 1,\n",
    "        is_SPT: bool = False,\n",
    "        is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeding = PatchEmbedding(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            emb_size=emb_size,\n",
    "            is_SPT=is_SPT,\n",
    "        )\n",
    "\n",
    "        self.transformer_encoders = TransformerEncoder(\n",
    "            depth=depth, emb_size=emb_size, num_heads=num_heads, is_LSA=is_LSA\n",
    "        )\n",
    "\n",
    "        self.classification_head = ClassificationHead(\n",
    "            emb_size=emb_size, n_classes=n_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeding(x)\n",
    "        x = self.transformer_encoders(x)\n",
    "        x = self.classification_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fca03-6496-4585-a477-3f8aec1efd5f",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Loading dataset\n",
    "\n",
    "Dataset choosen here is [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b9f9683-bbc7-4952-b834-f349346ebb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "transform = Compose(\n",
    "    [Resize((32, 32)), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494993a4-08d1-44e4-8c00-1eb8589eb6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # print labels\n",
    "# print(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcde706-7072-42dc-a7a8-cb762816ec13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a8a639-e386-4dce-9fdf-a87eb24f65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c3cd38f-d8e6-45fc-bef4-a9fb5825b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    in_channels=3,\n",
    "    img_size=32,\n",
    "    patch_size=16,\n",
    "    num_heads=8,\n",
    "    depth=3,\n",
    "    n_classes=10,\n",
    "    emb_size=512,\n",
    "    is_SPT=True,\n",
    "    is_LSA=True\n",
    ").to(\n",
    "    device\n",
    ")  # this should give us 4 patches for each images and only one encoder block with one attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65ae114-b03d-4fa4-85e7-ddd1f94c1999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 15:31:57.107307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-30 15:31:58.303096: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2023-01-30 15:31:58.303179: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2023-01-30 15:31:58.303188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/nathanh/.local/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3], device='cuda:0')\n",
      "tensor([[-9.4104e-01, -3.6240e-01, -5.6674e-01, -2.0025e-01,  1.5366e-01,\n",
      "          1.0087e+00,  1.0213e+00, -4.0411e-01,  7.4374e-01,  6.0833e-04]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor(2.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "testloader_b1 = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=False, num_workers=2\n",
    ")\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=0.05)\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=10).to(device);\n",
    "lr_decay = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "\n",
    "for img, label in testloader_b1:\n",
    "    img, label = (img.to(device), label.to(device))\n",
    "    y = model(img)\n",
    "    l = criterion(y, label)\n",
    "    acc = accuracy(y, label)\n",
    "    imshow(img.cpu()[0])\n",
    "    print(label)\n",
    "    print(y)\n",
    "    print(l)\n",
    "    print(acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e08cd20-3020-486e-a095-6641f0907199",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    device,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    lr_scheduler=lr_decay,\n",
    "    score_function=accuracy,\n",
    "    epochs=20,\n",
    "    notebook=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e116b2-a70b-40d4-9191-ca158c694764",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5d50eb2ffc4182a0e6d0c96903914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 1.656970).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.656970 --> 1.499501).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.499501 --> 1.434214).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.434214 --> 1.352954).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.352954 --> 1.318483).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.318483 --> 1.318483).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.318483 --> 1.315725).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.315725 --> 1.287213).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 7\n",
      "Early Stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.9207174297498197,\n",
       "  1.5648134727867282,\n",
       "  1.4260034293544537,\n",
       "  1.3054135052525266,\n",
       "  1.1836200453797165,\n",
       "  1.1315907075697063,\n",
       "  1.1351266938204667,\n",
       "  1.1817089103314342,\n",
       "  1.220575778460016,\n",
       "  1.2163694768535847,\n",
       "  1.1755515978652604,\n",
       "  1.0918175654143703,\n",
       "  0.9628480037256163,\n",
       "  0.7581421343647704,\n",
       "  0.5165698470509782,\n",
       "  0.4021998937336766,\n",
       "  0.4057231592584629,\n",
       "  0.535280744031984,\n",
       "  0.6820068113049682,\n",
       "  0.718873627331792],\n",
       " [1.6569699198007584,\n",
       "  1.499500721693039,\n",
       "  1.4342135936021805,\n",
       "  1.3529536843299865,\n",
       "  1.3184825599193573,\n",
       "  1.3184825599193573,\n",
       "  1.3157254934310914,\n",
       "  1.3354297012090683,\n",
       "  1.3284565567970277,\n",
       "  1.329351043701172,\n",
       "  1.3453693598508836,\n",
       "  1.335200560092926,\n",
       "  1.2872132450342177,\n",
       "  1.3635113477706908,\n",
       "  1.4665844827890395,\n",
       "  1.4665844827890395,\n",
       "  1.585568729043007,\n",
       "  1.5593200385570527,\n",
       "  1.460346508026123,\n",
       "  1.4796974033117294],\n",
       " [0.001,\n",
       "  0.0009045084971874737,\n",
       "  0.0006545084971874737,\n",
       "  0.00034549150281252633,\n",
       "  9.549150281252633e-05,\n",
       "  0.0,\n",
       "  9.549150281252627e-05,\n",
       "  0.0003454915028125264,\n",
       "  0.000654508497187474,\n",
       "  0.0009045084971874742,\n",
       "  0.0010000000000000005,\n",
       "  0.0009045084971874745,\n",
       "  0.0006545084971874742,\n",
       "  0.0003454915028125266,\n",
       "  9.549150281252642e-05,\n",
       "  0.0,\n",
       "  9.549150281252627e-05,\n",
       "  0.00034549150281252644,\n",
       "  0.0006545084971874743,\n",
       "  0.0009045084971874746])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "847a9546-7c1f-4dbe-8777-59e21785d937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../weights/checkpoint.pt\"))\n",
    "# trainer.save_training_metric(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b5218fe-282e-4ce0-938a-9866637e72c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39892578125,\n",
       " 0.46025390625,\n",
       " 0.47822265625,\n",
       " 0.51611328125,\n",
       " 0.5306640625,\n",
       " 0.5306640625,\n",
       " 0.5302734375,\n",
       " 0.527734375,\n",
       " 0.52724609375,\n",
       " 0.5220703125,\n",
       " 0.52333984375,\n",
       " 0.5296875,\n",
       " 0.5501953125,\n",
       " 0.56328125,\n",
       " 0.569921875,\n",
       " 0.569921875,\n",
       " 0.56435546875,\n",
       " 0.557421875,\n",
       " 0.55302734375,\n",
       " 0.5416015625]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e83edd-1b63-4e6a-8a0d-b3c3439cfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.520 with is_LSA 5 epochs\n",
    "# 0.527 with is_SPT 5 epochs\n",
    "# 0.522 with vanilla 5 epochs\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
