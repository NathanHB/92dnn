{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb49ee5-d5a6-48f5-90b2-c3b8af8cf702",
   "metadata": {},
   "source": [
    "# Vision Transformer encoder from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3bece6f-4e86-4fca-9dec-883c952d2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from PIL import Image\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "# from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Normalize, Resize, ToTensor\n",
    "\n",
    "from utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0361b7-57fa-4956-ac83-38dc4ebb23bf",
   "metadata": {},
   "source": [
    "### Patch embedding\n",
    "\n",
    "Using the rearange method, we have:\n",
    "\n",
    "* `b` the number of batches, here its one\n",
    "* `c` the number of channels, here its $3$\n",
    "* `(h s1)` is the total height ($224$), `s1` is the patch size ($16$) therefore `h` is $224/16$ which is $14$ \n",
    "* `(w s2)` is the total width ($224$), `s2` is the patch size ($16$) therefore `w` is $224/16$ which is $14$ \n",
    "\n",
    "We want to flatten all the patches.\n",
    "That means we will end up with:\n",
    "\n",
    "* `b`still the number of batch\n",
    "* `(h w)` the number of patches, therefore $14 * 14 = 196$ patches\n",
    "* `(s1 s2 c)` the size of the patches multiplied by the number of channels, that means each patch will be $768$ pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a77cce2-0d1c-47c6-afe3-3b0fdfe5d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        img_size: int,\n",
    "        patch_size: int = 16,\n",
    "        emb_size: int = 512,\n",
    "        is_SPT: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            img_size % patch_size == 0\n",
    "        ), f\"{img_size=}, needs to be a multiple of {patch_size=}\"\n",
    "\n",
    "        if is_SPT:\n",
    "            self.patch_shifting = PatchShifting(patch_size)\n",
    "            in_channels *= 5\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.to_patch = Rearrange(\n",
    "            \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_size, p2=patch_size\n",
    "        )\n",
    "        self.linear = nn.Linear(patch_size**2 * in_channels, emb_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(\n",
    "            torch.randn((img_size // patch_size) ** 2 + 1, emb_size)\n",
    "        )\n",
    "        self.is_SPT = is_SPT\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "\n",
    "        if self.is_SPT:\n",
    "            x = self.patch_shifting(x)\n",
    "\n",
    "        x = self.to_patch(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = repeat(self.cls_token, \"() n e -> b n e\", b=batch_size)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.positions\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9785ca0b-9242-4636-9aa6-b12b50efe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchShifting(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.shift = int(patch_size * (1 / 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_pad = torch.nn.functional.pad(\n",
    "            x, (self.shift, self.shift, self.shift, self.shift)\n",
    "        )\n",
    "        # if self.is_mean:\n",
    "        #     x_pad = x_pad.mean(dim=1, keepdim = True)\n",
    "\n",
    "        \"\"\" 4 cardinal directions \"\"\"\n",
    "        #############################\n",
    "        # x_l2 = x_pad[:, :, self.shift:-self.shift, :-self.shift*2]\n",
    "        # x_r2 = x_pad[:, :, self.shift:-self.shift, self.shift*2:]\n",
    "        # x_t2 = x_pad[:, :, :-self.shift*2, self.shift:-self.shift]\n",
    "        # x_b2 = x_pad[:, :, self.shift*2:, self.shift:-self.shift]\n",
    "        # x_cat = torch.cat([x, x_l2, x_r2, x_t2, x_b2], dim=1)\n",
    "        #############################\n",
    "\n",
    "        \"\"\" 4 diagonal directions \"\"\"\n",
    "        # #############################\n",
    "        x_lu = x_pad[:, :, : -self.shift * 2, : -self.shift * 2]\n",
    "        x_ru = x_pad[:, :, : -self.shift * 2, self.shift * 2 :]\n",
    "        x_lb = x_pad[:, :, self.shift * 2 :, : -self.shift * 2]\n",
    "        x_rb = x_pad[:, :, self.shift * 2 :, self.shift * 2 :]\n",
    "        out = torch.cat([x, x_lu, x_ru, x_lb, x_rb], dim=1)\n",
    "        # #############################\n",
    "\n",
    "        \"\"\" 8 cardinal directions \"\"\"\n",
    "        #############################\n",
    "        # x_l2 = x_pad[:, :, self.shift:-self.shift, :-self.shift*2]\n",
    "        # x_r2 = x_pad[:, :, self.shift:-self.shift, self.shift*2:]\n",
    "        # x_t2 = x_pad[:, :, :-self.shift*2, self.shift:-self.shift]\n",
    "        # x_b2 = x_pad[:, :, self.shift*2:, self.shift:-self.shift]\n",
    "        # x_lu = x_pad[:, :, :-self.shift*2, :-self.shift*2]\n",
    "        # x_ru = x_pad[:, :, :-self.shift*2, self.shift*2:]\n",
    "        # x_lb = x_pad[:, :, self.shift*2:, :-self.shift*2]\n",
    "        # x_rb = x_pad[:, :, self.shift*2:, self.shift*2:]\n",
    "        # x_cat = torch.cat([x, x_l2, x_r2, x_t2, x_b2, x_lu, x_ru, x_lb, x_rb], dim=1)\n",
    "        #############################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d21b59-90b5-4902-87ce-9b2f748c5a61",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "the attention takes three inputs, the famous queries, keys, and values, and computes the attention matrix using queries and values and use it to “attend” to the values.\n",
    "\n",
    "First step is to split key, queries and values, we will go step by step for queries, keys and values follow the same pattern.\n",
    "* The input go trough a linear layer\n",
    "* we rearange the input\n",
    "    * `b` the batch size\n",
    "    * `n` the number of patches\n",
    "    * `(h d)` the size of one patch where `h` is the number of heads so `d` is the size of a patch given to an attention head.\n",
    "    * We rearange that into, the same batch size, but now we have `h` patches that each will handle the full amount of patches but of size `d`\n",
    "    \n",
    "Now that we have our `keys` `values` and `queries` in the right shape, we can perform a `matrix multiplication` between the `keys` and the `queries`.\n",
    "\n",
    "The attention map is then the softmax of this resulting matrix divided by some scaling.\n",
    "\n",
    "We parform a matrix multiplication and rearange the result back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e2d12b1-14a1-4dc9-a23d-7f4172bb3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        emb_size: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "        self.scaling = emb_size ** (1 / 2)\n",
    "        self.is_LSA = is_LSA\n",
    "\n",
    "        if is_LSA:\n",
    "            num_patches = 5\n",
    "            self.scaling = nn.Parameter(self.scaling * torch.ones(1))\n",
    "            self.mask = torch.eye(num_patches, num_patches)\n",
    "            self.mask = torch.nonzero((self.mask == 1), as_tuple=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # split keys, queries and values in num_heads\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "\n",
    "        # sum up over the last axis\n",
    "        attention_weights = torch.einsum(\n",
    "            \"bhqd, bhkd -> bhqk\", queries, keys\n",
    "        )  # batch, num_heads, query_len, key_len\n",
    "\n",
    "        if self.is_LSA:\n",
    "            attention_weights[:, :, self.mask[:, 0], self.mask[:, 1]] = -987654321\n",
    "\n",
    "        attention_weights = F.softmax(attention_weights / self.scaling, dim=-1)\n",
    "        attention_weights = self.att_drop(attention_weights)\n",
    "\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum(\"bhal, bhlv -> bhav \", attention_weights, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")  # concat the heads\n",
    "        out = self.projection(out)  # project out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb45f3-c5f5-4dc2-9064-3416b4892710",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f310bb12-3e3b-4d37-b5f5-cf8ce1db9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer Encoder Block is a layerNorm foloowed by a MultiHeadAttention,\n",
    "    another layerNorm and finally a linearLayer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, emb_size: int = 512, num_heads: int = 8, is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            emb_size=emb_size, num_heads=num_heads, is_LSA=is_LSA\n",
    "        )\n",
    "        self.layer_norm_1 = LayerNorm(emb_size)\n",
    "        self.layer_norm_2 = LayerNorm(emb_size)\n",
    "        dropout = 0.1\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * emb_size, emb_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        residual = x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c73ff33f-29a0-491b-9328-e6082aaed88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Consists of `depth` encoder blocks chained together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth: int = 1,\n",
    "        emb_size: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(emb_size=emb_size, num_heads=num_heads, is_LSA=is_LSA)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491994e-4190-46de-b12e-6982c406122b",
   "metadata": {},
   "source": [
    "### Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b286f2d-a1fb-4a76-9e57-6946f6517035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, *, emb_size: int = 512, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce(\n",
    "                \"b n e -> b e\", reduction=\"mean\"\n",
    "            ),  # Reduce the N patches into One patch by averaging them\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eef675-cb09-44ce-b7cd-d53985fbb990",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a41cc707-353c-4a92-952f-76b22c54d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels: int,\n",
    "        img_size: int,\n",
    "        n_classes: int = 10,\n",
    "        patch_size: int = 8,\n",
    "        emb_size: int = 192,\n",
    "        num_heads=12,\n",
    "        depth: int = 1,\n",
    "        is_SPT: bool = False,\n",
    "        is_LSA: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embeding = PatchEmbedding(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            emb_size=emb_size,\n",
    "            is_SPT=is_SPT,\n",
    "        )\n",
    "\n",
    "        self.transformer_encoders = TransformerEncoder(\n",
    "            depth=depth, emb_size=emb_size, num_heads=num_heads, is_LSA=is_LSA\n",
    "        )\n",
    "\n",
    "        self.classification_head = ClassificationHead(\n",
    "            emb_size=emb_size, n_classes=n_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeding(x)\n",
    "        x = self.transformer_encoders(x)\n",
    "        x = self.classification_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fca03-6496-4585-a477-3f8aec1efd5f",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Loading dataset\n",
    "\n",
    "Dataset choosen here is [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b9f9683-bbc7-4952-b834-f349346ebb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from autoaugment import CIFAR10Policy\n",
    "\n",
    "batch_size = 256\n",
    "transform = Compose(\n",
    "    [Resize((32, 32)), CIFAR10Policy(), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "494993a4-08d1-44e4-8c00-1eb8589eb6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(24, 5))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # print labels\n",
    "# print(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcde706-7072-42dc-a7a8-cb762816ec13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25a8a639-e386-4dce-9fdf-a87eb24f65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c3cd38f-d8e6-45fc-bef4-a9fb5825b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    in_channels=3,\n",
    "    img_size=32,\n",
    "    patch_size=16,\n",
    "    num_heads=8,\n",
    "    depth=3,\n",
    "    n_classes=10,\n",
    "    emb_size=512,\n",
    "    is_SPT=False,\n",
    "    is_LSA=False\n",
    ").to(\n",
    "    device\n",
    ")  # this should give us 4 patches for each images and only one encoder block with one attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c65ae114-b03d-4fa4-85e7-ddd1f94c1999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3], device='cuda:0')\n",
      "tensor([[ 1.1568, -0.2250, -0.4941,  1.0923, -0.4997, -0.0336, -1.0338,  0.2099,\n",
      "         -0.2053, -0.0790]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor(1.4311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "testloader_b1 = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=False, num_workers=2\n",
    ")\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.003, weight_decay=0.05)\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=10).to(device);\n",
    "lr_decay = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "\n",
    "for img, label in testloader_b1:\n",
    "    img, label = (img.to(device), label.to(device))\n",
    "    y = model(img)\n",
    "    l = criterion(y, label)\n",
    "    acc = accuracy(y, label)\n",
    "    imshow(img.cpu()[0])\n",
    "    print(label)\n",
    "    print(y)\n",
    "    print(l)\n",
    "    print(acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e08cd20-3020-486e-a095-6641f0907199",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    device,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    lr_scheduler=lr_decay,\n",
    "    score_function=accuracy,\n",
    "    epochs=100,\n",
    "    notebook=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33e116b2-a70b-40d4-9191-ca158c694764",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf729e8089f146899932482c52d9bc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 1.541337).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.541337 --> 1.501003).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.501003 --> 1.477599).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.477599 --> 1.445690).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.445690 --> 1.420750).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.420750 --> 1.412088).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 7\n",
      "Early Stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.2513467535680656,\n",
       "  2.0312843845815074,\n",
       "  1.959501955582171,\n",
       "  1.8899368856634413,\n",
       "  1.8310776267732893,\n",
       "  1.8143795765176112,\n",
       "  1.810422662569552,\n",
       "  1.8296628922832257,\n",
       "  1.8263164880324383,\n",
       "  1.8153897572536857,\n",
       "  1.7899872946495912,\n",
       "  1.7477599376318407,\n",
       "  1.6941319971668476,\n",
       "  1.6357562724424868,\n",
       "  1.5764095619016765,\n",
       "  1.559282262106331,\n",
       "  1.5624479663615325,\n",
       "  1.5828123670451495,\n",
       "  1.6187350567506285,\n",
       "  1.6268041991457647,\n",
       "  1.6103533378669195,\n",
       "  1.5737698394425061,\n",
       "  1.5138169703434925,\n",
       "  1.4517904325407378,\n",
       "  1.3799497923072503,\n",
       "  1.3588830494150823,\n",
       "  1.35701214902255,\n",
       "  1.3940464702187751,\n",
       "  1.4422228895888036,\n",
       "  1.4643902602244396,\n",
       "  1.4465952175004142,\n",
       "  1.413608607588982,\n",
       "  1.3509999160863915,\n",
       "  1.269441430057798,\n",
       "  1.1814248011428483,\n",
       "  1.155617586508089,\n",
       "  1.1539978053496809,\n",
       "  1.193910792165873,\n",
       "  1.251020266085255,\n",
       "  1.2919492904020815,\n",
       "  1.2922489162610502,\n",
       "  1.2579330230245784,\n",
       "  1.1864526174506362,\n",
       "  1.08028132240383],\n",
       " [2.090595379471779,\n",
       "  1.9763971269130707,\n",
       "  1.9411967486143111,\n",
       "  1.8513615250587463,\n",
       "  1.8150554805994035,\n",
       "  1.824517947435379,\n",
       "  1.8218451976776122,\n",
       "  1.8228376179933548,\n",
       "  1.8289703458547593,\n",
       "  1.804311576485634,\n",
       "  1.7809267312288284,\n",
       "  1.753576037287712,\n",
       "  1.707886204123497,\n",
       "  1.6499202370643615,\n",
       "  1.6194022744894028,\n",
       "  1.6135158717632294,\n",
       "  1.607118758559227,\n",
       "  1.6051615327596664,\n",
       "  1.6340814471244811,\n",
       "  1.6574317932128906,\n",
       "  1.6313599854707719,\n",
       "  1.6088860124349593,\n",
       "  1.5648031324148177,\n",
       "  1.5116297215223313,\n",
       "  1.507613515853882,\n",
       "  1.4766616702079773,\n",
       "  1.4896257907152175,\n",
       "  1.5069726526737213,\n",
       "  1.532143458724022,\n",
       "  1.5432823866605758,\n",
       "  1.5413367688655852,\n",
       "  1.5010034292936325,\n",
       "  1.4775989174842834,\n",
       "  1.4456896483898163,\n",
       "  1.4207502812147141,\n",
       "  1.424740305542946,\n",
       "  1.4120881974697113,\n",
       "  1.421010398864746,\n",
       "  1.4697256892919541,\n",
       "  1.5119653403759004,\n",
       "  1.4621168822050095,\n",
       "  1.4641694903373719,\n",
       "  1.4386640667915345,\n",
       "  1.4179754495620727],\n",
       " [0.003,\n",
       "  0.002713525491562421,\n",
       "  0.001963525491562421,\n",
       "  0.0010364745084375786,\n",
       "  0.00028647450843757887,\n",
       "  0.0,\n",
       "  0.0002864745084375788,\n",
       "  0.0010364745084375793,\n",
       "  0.001963525491562422,\n",
       "  0.0027135254915624228,\n",
       "  0.0030000000000000014,\n",
       "  0.002713525491562423,\n",
       "  0.001963525491562422,\n",
       "  0.0010364745084375797,\n",
       "  0.00028647450843757925,\n",
       "  0.0,\n",
       "  0.0002864745084375788,\n",
       "  0.0010364745084375793,\n",
       "  0.0019635254915624225,\n",
       "  0.002713525491562423,\n",
       "  0.003000000000000003,\n",
       "  0.0027135254915624245,\n",
       "  0.001963525491562427,\n",
       "  0.0010364745084375808,\n",
       "  0.0002864745084375797,\n",
       "  0.0,\n",
       "  0.0002864745084375788,\n",
       "  0.0010364745084375797,\n",
       "  0.001963525491562424,\n",
       "  0.002713525491562426,\n",
       "  0.003000000000000006,\n",
       "  0.0027135254915624275,\n",
       "  0.0019635254915624264,\n",
       "  0.0010364745084375823,\n",
       "  0.00028647450843758034,\n",
       "  0.0,\n",
       "  0.0002864745084375788,\n",
       "  0.00103647450843758,\n",
       "  0.0019635254915624243,\n",
       "  0.0027135254915624267,\n",
       "  0.003000000000000007,\n",
       "  0.0027135254915624314,\n",
       "  0.001963525491562427,\n",
       "  0.0010364745084375827])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "847a9546-7c1f-4dbe-8777-59e21785d937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../weights/checkpoint.pt\"))\n",
    "# trainer.save_training_metric(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b5218fe-282e-4ce0-938a-9866637e72c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2234375,\n",
       " 0.2728515625,\n",
       " 0.30283203125,\n",
       " 0.3291015625,\n",
       " 0.34423828125,\n",
       " 0.34677734375,\n",
       " 0.3515625,\n",
       " 0.34052734375,\n",
       " 0.34208984375,\n",
       " 0.34853515625,\n",
       " 0.35234375,\n",
       " 0.36796875,\n",
       " 0.3880859375,\n",
       " 0.4064453125,\n",
       " 0.41767578125,\n",
       " 0.4154296875,\n",
       " 0.42138671875,\n",
       " 0.42587890625,\n",
       " 0.4119140625,\n",
       " 0.40390625,\n",
       " 0.41787109375,\n",
       " 0.41904296875,\n",
       " 0.44150390625,\n",
       " 0.45810546875,\n",
       " 0.45634765625,\n",
       " 0.4724609375,\n",
       " 0.4705078125,\n",
       " 0.46396484375,\n",
       " 0.44921875,\n",
       " 0.4396484375,\n",
       " 0.4482421875,\n",
       " 0.4640625,\n",
       " 0.473828125,\n",
       " 0.4951171875,\n",
       " 0.50498046875,\n",
       " 0.501953125,\n",
       " 0.510546875,\n",
       " 0.49208984375,\n",
       " 0.47646484375,\n",
       " 0.4638671875,\n",
       " 0.48603515625,\n",
       " 0.4810546875,\n",
       " 0.49482421875,\n",
       " 0.51181640625]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e83edd-1b63-4e6a-8a0d-b3c3439cfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.520 with is_LSA 5 epochs\n",
    "# 0.527 with is_SPT 5 epochs\n",
    "# 0.522 with vanilla 5 epochs\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
